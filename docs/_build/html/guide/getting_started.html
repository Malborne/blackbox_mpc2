

<!DOCTYPE html>
<html class="writer-html5" lang="python" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Getting Started &mdash; blackbox_mpc 0.3 documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="blackbox_mpc" href="../modules/blackbox_mpc.html" />
    <link rel="prev" title="Installation" href="install.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> blackbox_mpc
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Guide</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="install.html">Installation</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Getting Started</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#model-predictive-control-with-true-model">Model Predictive Control with True Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="#learn-the-dynamics-model">Learn the Dynamics Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="#modelbased-rl">ModelBased RL</a></li>
<li class="toctree-l2"><a class="reference internal" href="#load-save-models">Load/ Save Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="#record-videos">Record Videos</a></li>
<li class="toctree-l2"><a class="reference internal" href="#using-low-level-api">Using low-level API</a></li>
<li class="toctree-l2"><a class="reference internal" href="#halfcheetah">HalfCheetah</a></li>
<li class="toctree-l2"><a class="reference internal" href="#more">More</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../modules/blackbox_mpc.html">blackbox_mpc</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">blackbox_mpc</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Getting Started</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/guide/getting_started.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="getting-started">
<span id="id1"></span><h1>Getting Started<a class="headerlink" href="#getting-started" title="Permalink to this headline">¶</a></h1>
<p>The easiest way to get familiar with the framework is to run through the tutorials below.</p>
<div class="section" id="model-predictive-control-with-true-model">
<h2>Model Predictive Control with True Model<a class="headerlink" href="#model-predictive-control-with-true-model" title="Permalink to this headline">¶</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">- instantiate an env for a pendulum</span>
<span class="sd">- instantiate an MPC controller using the true known analytical model</span>
<span class="sd">- define cost/reward functions as used in the openAI gym env.</span>
<span class="sd">- render the resulting MPC afterwards</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">blackbox_mpc.policies.mpc_policy</span> <span class="kn">import</span> \
    <span class="n">MPCPolicy</span>
<span class="kn">from</span> <span class="nn">blackbox_mpc.utils.pendulum</span> <span class="kn">import</span> <span class="n">PendulumTrueModel</span><span class="p">,</span> \
    <span class="n">pendulum_reward_function</span>
<span class="kn">import</span> <span class="nn">gym</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s2">&quot;Pendulum-v0&quot;</span><span class="p">)</span>
<span class="n">mpc_policy</span> <span class="o">=</span> <span class="n">MPCPolicy</span><span class="p">(</span><span class="n">reward_function</span><span class="o">=</span><span class="n">pendulum_reward_function</span><span class="p">,</span>
                       <span class="n">env_action_space</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="p">,</span>
                       <span class="n">env_observation_space</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="p">,</span>
                       <span class="n">true_model</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                       <span class="n">dynamics_function</span><span class="o">=</span><span class="n">PendulumTrueModel</span><span class="p">(),</span>
                       <span class="n">optimizer_name</span><span class="o">=</span><span class="s1">&#39;RandomSearch&#39;</span><span class="p">,</span>
                       <span class="n">num_agents</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">current_obs</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">200</span><span class="p">):</span>
    <span class="n">action_to_execute</span><span class="p">,</span> <span class="n">expected_obs</span><span class="p">,</span> <span class="n">expected_reward</span> <span class="o">=</span> <span class="n">mpc_policy</span><span class="o">.</span><span class="n">act</span><span class="p">(</span>
        <span class="n">current_obs</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
    <span class="n">current_obs</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action_to_execute</span><span class="p">)</span>
    <span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">- instantiate an env for a pendulum</span>
<span class="sd">- instantiate an MPC controller using the true known analytical model</span>
<span class="sd">- define cost/reward functions as used in the openAI gym env.</span>
<span class="sd">- render the resulting MPC afterwards</span>
<span class="sd">- switch optimizer</span>
<span class="sd">- render the resulting MPC afterwards and compare</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">blackbox_mpc.policies.mpc_policy</span> <span class="kn">import</span> \
    <span class="n">MPCPolicy</span>
<span class="kn">from</span> <span class="nn">blackbox_mpc.utils.pendulum</span> <span class="kn">import</span> <span class="n">PendulumTrueModel</span><span class="p">,</span> \
    <span class="n">pendulum_reward_function</span>
<span class="kn">import</span> <span class="nn">gym</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s2">&quot;Pendulum-v0&quot;</span><span class="p">)</span>
<span class="n">mpc_policy</span> <span class="o">=</span> <span class="n">MPCPolicy</span><span class="p">(</span><span class="n">reward_function</span><span class="o">=</span><span class="n">pendulum_reward_function</span><span class="p">,</span>
                       <span class="n">env_action_space</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="p">,</span>
                       <span class="n">env_observation_space</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="p">,</span>
                       <span class="n">true_model</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                       <span class="n">dynamics_function</span><span class="o">=</span><span class="n">PendulumTrueModel</span><span class="p">(),</span>
                       <span class="n">optimizer_name</span><span class="o">=</span><span class="s1">&#39;CEM&#39;</span><span class="p">,</span>
                       <span class="n">num_agents</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">current_obs</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">200</span><span class="p">):</span>
    <span class="n">action_to_execute</span><span class="p">,</span> <span class="n">expected_obs</span><span class="p">,</span> <span class="n">expected_reward</span> <span class="o">=</span> <span class="n">mpc_policy</span><span class="o">.</span><span class="n">act</span><span class="p">(</span>
        <span class="n">current_obs</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
    <span class="n">current_obs</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action_to_execute</span><span class="p">)</span>
    <span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">()</span>

<span class="n">mpc_policy</span><span class="o">.</span><span class="n">switch_optimizer</span><span class="p">(</span><span class="n">optimizer_name</span><span class="o">=</span><span class="s1">&#39;RandomSearch&#39;</span><span class="p">)</span>

<span class="n">current_obs</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">200</span><span class="p">):</span>
    <span class="n">action_to_execute</span><span class="p">,</span> <span class="n">expected_obs</span><span class="p">,</span> <span class="n">expected_reward</span> <span class="o">=</span> <span class="n">mpc_policy</span><span class="o">.</span><span class="n">act</span><span class="p">(</span>
        <span class="n">current_obs</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
    <span class="n">current_obs</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action_to_execute</span><span class="p">)</span>
    <span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">- instantiate an env for a pendulum</span>
<span class="sd">- instantiate an MPC controller using the true known analytical model</span>
<span class="sd">- define cost/reward functions as used in the openAI gym env.</span>
<span class="sd">- collect some rollouts and then use them as u like afterwards.</span>
<span class="sd">- record everything in tensorboard.</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">blackbox_mpc.policies.mpc_policy</span> <span class="kn">import</span> \
    <span class="n">MPCPolicy</span>
<span class="kn">from</span> <span class="nn">blackbox_mpc.utils.pendulum</span> <span class="kn">import</span> <span class="n">PendulumTrueModel</span><span class="p">,</span> \
    <span class="n">pendulum_reward_function</span>
<span class="kn">from</span> <span class="nn">blackbox_mpc.utils.rollouts</span> <span class="kn">import</span> <span class="n">perform_rollouts</span>
<span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">from</span> <span class="nn">blackbox_mpc.environment_utils</span> <span class="kn">import</span> <span class="n">EnvironmentWrapper</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s2">&quot;Pendulum-v0&quot;</span><span class="p">)</span>
<span class="n">log_dir</span> <span class="o">=</span> <span class="s1">&#39;./&#39;</span>
<span class="n">tf_writer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">create_file_writer</span><span class="p">(</span><span class="n">log_dir</span><span class="p">)</span>
<span class="n">mpc_policy</span> <span class="o">=</span> <span class="n">MPCPolicy</span><span class="p">(</span><span class="n">reward_function</span><span class="o">=</span><span class="n">pendulum_reward_function</span><span class="p">,</span>
                       <span class="n">env_action_space</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="p">,</span>
                       <span class="n">env_observation_space</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="p">,</span>
                       <span class="n">true_model</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                       <span class="n">dynamics_function</span><span class="o">=</span><span class="n">PendulumTrueModel</span><span class="p">(),</span>
                       <span class="n">optimizer_name</span><span class="o">=</span><span class="s1">&#39;CEM&#39;</span><span class="p">,</span>
                       <span class="n">tf_writer</span><span class="o">=</span><span class="n">tf_writer</span><span class="p">,</span>
                       <span class="n">log_dir</span><span class="o">=</span><span class="n">log_dir</span><span class="p">,</span>
                       <span class="n">num_agents</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">traj_obs</span><span class="p">,</span> <span class="n">traj_acs</span><span class="p">,</span> <span class="n">traj_rews</span> <span class="o">=</span>\
    <span class="n">perform_rollouts</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">EnvironmentWrapper</span><span class="o">.</span><span class="n">make_standard_gym_env</span><span class="p">(</span><span class="s2">&quot;Pendulum-v0&quot;</span><span class="p">,</span>
                                                              <span class="n">num_of_agents</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
                     <span class="n">number_of_rollouts</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                     <span class="n">task_horizon</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
                     <span class="n">policy</span><span class="o">=</span><span class="n">mpc_policy</span><span class="p">,</span>
                     <span class="n">exploration_noise</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                     <span class="n">tf_writer</span><span class="o">=</span><span class="n">tf_writer</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="learn-the-dynamics-model">
<h2>Learn the Dynamics Model<a class="headerlink" href="#learn-the-dynamics-model" title="Permalink to this headline">¶</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">- instantiate an env for a pendulum</span>
<span class="sd">- Define an MLP to learn a dynamics model</span>
<span class="sd">- instantiate a random policy to collect rollouts</span>
<span class="sd">- learn dynamics after collecting rollouts randomly</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">blackbox_mpc.dynamics_functions.deterministic_mlp</span> <span class="kn">import</span> \
    <span class="n">DeterministicMLP</span>
<span class="kn">from</span> <span class="nn">blackbox_mpc.policies.random_policy</span> <span class="kn">import</span> <span class="n">RandomPolicy</span>
<span class="kn">from</span> <span class="nn">blackbox_mpc.utils.dynamics_learning</span> <span class="kn">import</span> <span class="n">learn_dynamics_from_policy</span>
<span class="kn">from</span> <span class="nn">blackbox_mpc.environment_utils</span> <span class="kn">import</span> <span class="n">EnvironmentWrapper</span>
<span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="n">log_path</span> <span class="o">=</span> <span class="s1">&#39;./tutorial_2&#39;</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s2">&quot;Pendulum-v0&quot;</span><span class="p">)</span>
<span class="n">dynamics_function</span> <span class="o">=</span> <span class="n">DeterministicMLP</span><span class="p">(</span><span class="n">layers</span><span class="o">=</span><span class="p">[</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                                              <span class="mi">32</span><span class="p">,</span>
                                              <span class="mi">32</span><span class="p">,</span>
                                              <span class="mi">32</span><span class="p">,</span>
                                              <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span>
                                     <span class="n">activation_functions</span><span class="o">=</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">tanh</span><span class="p">,</span>
                                                           <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">tanh</span><span class="p">,</span>
                                                           <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">tanh</span><span class="p">,</span>
                                                           <span class="kc">None</span><span class="p">])</span>
<span class="n">policy</span> <span class="o">=</span> <span class="n">RandomPolicy</span><span class="p">(</span><span class="n">number_of_agents</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                      <span class="n">env_action_space</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="p">)</span>
<span class="n">dynamics_handler</span> <span class="o">=</span> <span class="n">learn_dynamics_from_policy</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">EnvironmentWrapper</span><span class="o">.</span><span class="n">make_standard_gym_env</span><span class="p">(</span><span class="s2">&quot;Pendulum-v0&quot;</span><span class="p">,</span>
                                                                                           <span class="n">num_of_agents</span><span class="o">=</span><span class="mi">10</span><span class="p">),</span>
                                              <span class="n">policy</span><span class="o">=</span><span class="n">policy</span><span class="p">,</span>
                                              <span class="n">number_of_rollouts</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                                              <span class="n">task_horizon</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
                                              <span class="n">dynamics_function</span><span class="o">=</span><span class="n">dynamics_function</span><span class="p">)</span>

</pre></div>
</div>
</div>
<div class="section" id="modelbased-rl">
<h2>ModelBased RL<a class="headerlink" href="#modelbased-rl" title="Permalink to this headline">¶</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">- instantiate an env for a pendulum</span>
<span class="sd">- Define an MLP to learn a dynamics model</span>
<span class="sd">- instantiate a random policy to collect rollouts</span>
<span class="sd">- learn dynamics after collecting rollouts randomly</span>
<span class="sd">- plug in the dynamics model in an MPC</span>
<span class="sd">- render the result</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">blackbox_mpc.dynamics_functions.deterministic_mlp</span> <span class="kn">import</span> \
    <span class="n">DeterministicMLP</span>
<span class="kn">from</span> <span class="nn">blackbox_mpc.policies.random_policy</span> <span class="kn">import</span> <span class="n">RandomPolicy</span>
<span class="kn">from</span> <span class="nn">blackbox_mpc.policies.mpc_policy</span> <span class="kn">import</span> \
    <span class="n">MPCPolicy</span>
<span class="kn">from</span> <span class="nn">blackbox_mpc.utils.dynamics_learning</span> <span class="kn">import</span> <span class="n">learn_dynamics_from_policy</span>
<span class="kn">from</span> <span class="nn">blackbox_mpc.environment_utils</span> <span class="kn">import</span> <span class="n">EnvironmentWrapper</span>
<span class="kn">from</span> <span class="nn">blackbox_mpc.utils.pendulum</span> <span class="kn">import</span> <span class="n">pendulum_reward_function</span>
<span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s2">&quot;Pendulum-v0&quot;</span><span class="p">)</span>
<span class="n">dynamics_function</span> <span class="o">=</span> <span class="n">DeterministicMLP</span><span class="p">(</span><span class="n">layers</span><span class="o">=</span><span class="p">[</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                                              <span class="mi">32</span><span class="p">,</span>
                                              <span class="mi">32</span><span class="p">,</span>
                                              <span class="mi">32</span><span class="p">,</span>
                                              <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span>
                                     <span class="n">activation_functions</span><span class="o">=</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">tanh</span><span class="p">,</span>
                                                           <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">tanh</span><span class="p">,</span>
                                                           <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">tanh</span><span class="p">,</span>
                                                           <span class="kc">None</span><span class="p">])</span>
<span class="n">policy</span> <span class="o">=</span> <span class="n">RandomPolicy</span><span class="p">(</span><span class="n">number_of_agents</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                      <span class="n">env_action_space</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="p">)</span>
<span class="n">dynamics_handler</span> <span class="o">=</span> <span class="n">learn_dynamics_from_policy</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">EnvironmentWrapper</span><span class="o">.</span><span class="n">make_standard_gym_env</span><span class="p">(</span><span class="s2">&quot;Pendulum-v0&quot;</span><span class="p">,</span>
                                                                                           <span class="n">num_of_agents</span><span class="o">=</span><span class="mi">10</span><span class="p">),</span>
                                              <span class="n">policy</span><span class="o">=</span><span class="n">policy</span><span class="p">,</span>
                                              <span class="n">number_of_rollouts</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                                              <span class="n">task_horizon</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
                                              <span class="n">dynamics_function</span><span class="o">=</span><span class="n">dynamics_function</span><span class="p">)</span>
<span class="n">mpc_policy</span> <span class="o">=</span> <span class="n">MPCPolicy</span><span class="p">(</span><span class="n">reward_function</span><span class="o">=</span><span class="n">pendulum_reward_function</span><span class="p">,</span>
                       <span class="n">env_action_space</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="p">,</span>
                       <span class="n">env_observation_space</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="p">,</span>
                       <span class="n">dynamics_handler</span><span class="o">=</span><span class="n">dynamics_handler</span><span class="p">,</span>
                       <span class="n">optimizer_name</span><span class="o">=</span><span class="s1">&#39;CEM&#39;</span><span class="p">)</span>

<span class="n">current_obs</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">200</span><span class="p">):</span>
    <span class="n">action_to_execute</span><span class="p">,</span> <span class="n">expected_obs</span><span class="p">,</span> <span class="n">expected_reward</span> <span class="o">=</span> <span class="n">mpc_policy</span><span class="o">.</span><span class="n">act</span><span class="p">(</span>
        <span class="n">current_obs</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
    <span class="n">current_obs</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action_to_execute</span><span class="p">)</span>
    <span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">- instantiate an env for a pendulum</span>
<span class="sd">- Define an MLP to learn a dynamics model</span>
<span class="sd">- instantiate a random policy to collect rollouts</span>
<span class="sd">- learn dynamics in an iterative mpc fashion</span>
<span class="sd">(collect -&gt; learn model -&gt; collect using mpc with learned model -&gt; repeat)</span>
<span class="sd">- record everything in tensorboard</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">blackbox_mpc.dynamics_functions.deterministic_mlp</span> <span class="kn">import</span> \
    <span class="n">DeterministicMLP</span>
<span class="kn">from</span> <span class="nn">blackbox_mpc.policies.random_policy</span> <span class="kn">import</span> <span class="n">RandomPolicy</span>
<span class="kn">from</span> <span class="nn">blackbox_mpc.utils.iterative_mpc</span> <span class="kn">import</span> <span class="n">learn_dynamics_iteratively_w_mpc</span>
<span class="kn">from</span> <span class="nn">blackbox_mpc.environment_utils</span> <span class="kn">import</span> <span class="n">EnvironmentWrapper</span>
<span class="kn">from</span> <span class="nn">blackbox_mpc.utils.pendulum</span> <span class="kn">import</span> <span class="n">pendulum_reward_function</span>
<span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="n">log_dir</span> <span class="o">=</span> <span class="s1">&#39;./&#39;</span>
<span class="n">tf_writer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">create_file_writer</span><span class="p">(</span><span class="n">log_dir</span><span class="p">)</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s2">&quot;Pendulum-v0&quot;</span><span class="p">)</span>
<span class="n">dynamics_function</span> <span class="o">=</span> <span class="n">DeterministicMLP</span><span class="p">(</span><span class="n">layers</span><span class="o">=</span><span class="p">[</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span>
                                             <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                                              <span class="mi">32</span><span class="p">,</span>
                                              <span class="mi">32</span><span class="p">,</span>
                                              <span class="mi">32</span><span class="p">,</span>
                                              <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span>
                                     <span class="n">activation_functions</span><span class="o">=</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">tanh</span><span class="p">,</span>
                                                           <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">tanh</span><span class="p">,</span>
                                                           <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">tanh</span><span class="p">,</span>
                                                           <span class="kc">None</span><span class="p">])</span>
<span class="n">initial_policy</span> <span class="o">=</span> <span class="n">RandomPolicy</span><span class="p">(</span><span class="n">number_of_agents</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                              <span class="n">env_action_space</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="p">)</span>

<span class="n">learn_dynamics_iteratively_w_mpc</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">EnvironmentWrapper</span><span class="o">.</span><span class="n">make_standard_gym_env</span><span class="p">(</span>
    <span class="s2">&quot;Pendulum-v0&quot;</span><span class="p">,</span> <span class="n">num_of_agents</span><span class="o">=</span><span class="mi">10</span><span class="p">),</span>
                                 <span class="n">env_action_space</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="p">,</span>
                                 <span class="n">env_observation_space</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="p">,</span>
                                 <span class="n">number_of_initial_rollouts</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                                 <span class="n">number_of_rollouts_for_refinement</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                                 <span class="n">number_of_refinement_steps</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                                 <span class="n">task_horizon</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
                                 <span class="n">planning_horizon</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
                                 <span class="n">initial_policy</span><span class="o">=</span><span class="n">initial_policy</span><span class="p">,</span>
                                 <span class="n">dynamics_function</span><span class="o">=</span><span class="n">dynamics_function</span><span class="p">,</span>
                                 <span class="n">num_agents</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                                 <span class="n">reward_function</span><span class="o">=</span><span class="n">pendulum_reward_function</span><span class="p">,</span>
                                 <span class="n">log_dir</span><span class="o">=</span><span class="n">log_dir</span><span class="p">,</span>
                                 <span class="n">tf_writer</span><span class="o">=</span><span class="n">tf_writer</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">- instantiate an env for a pendulum</span>
<span class="sd">- Define an MLP to learn a dynamics model</span>
<span class="sd">- instantiate a random policy to collect rollouts</span>
<span class="sd">- learn dynamics model from random rollouts</span>
<span class="sd">- render the result using mpc to control</span>
<span class="sd">- learn dynamics model from mpc rollouts</span>
<span class="sd">- render the result using mpc to control</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">blackbox_mpc.dynamics_functions.deterministic_mlp</span> <span class="kn">import</span> \
    <span class="n">DeterministicMLP</span>
<span class="kn">from</span> <span class="nn">blackbox_mpc.policies.random_policy</span> <span class="kn">import</span> <span class="n">RandomPolicy</span>
<span class="kn">from</span> <span class="nn">blackbox_mpc.policies.mpc_policy</span> <span class="kn">import</span> \
    <span class="n">MPCPolicy</span>
<span class="kn">from</span> <span class="nn">blackbox_mpc.utils.dynamics_learning</span> <span class="kn">import</span> <span class="n">learn_dynamics_from_policy</span>
<span class="kn">from</span> <span class="nn">blackbox_mpc.environment_utils</span> <span class="kn">import</span> <span class="n">EnvironmentWrapper</span>
<span class="kn">from</span> <span class="nn">blackbox_mpc.utils.pendulum</span> <span class="kn">import</span> <span class="n">pendulum_reward_function</span>
<span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s2">&quot;Pendulum-v0&quot;</span><span class="p">)</span>
<span class="n">dynamics_function</span> <span class="o">=</span> <span class="n">DeterministicMLP</span><span class="p">(</span><span class="n">layers</span><span class="o">=</span><span class="p">[</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                                              <span class="mi">32</span><span class="p">,</span>
                                              <span class="mi">32</span><span class="p">,</span>
                                              <span class="mi">32</span><span class="p">,</span>
                                              <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span>
                                     <span class="n">activation_functions</span><span class="o">=</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">tanh</span><span class="p">,</span>
                                                           <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">tanh</span><span class="p">,</span>
                                                           <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">tanh</span><span class="p">,</span>
                                                           <span class="kc">None</span><span class="p">])</span>
<span class="n">policy</span> <span class="o">=</span> <span class="n">RandomPolicy</span><span class="p">(</span><span class="n">number_of_agents</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                      <span class="n">env_action_space</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="p">)</span>
<span class="n">dynamics_handler</span> <span class="o">=</span> <span class="n">learn_dynamics_from_policy</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">EnvironmentWrapper</span><span class="o">.</span><span class="n">make_standard_gym_env</span><span class="p">(</span><span class="s2">&quot;Pendulum-v0&quot;</span><span class="p">,</span>
                                                                                           <span class="n">num_of_agents</span><span class="o">=</span><span class="mi">10</span><span class="p">),</span>
                                              <span class="n">policy</span><span class="o">=</span><span class="n">policy</span><span class="p">,</span>
                                              <span class="n">number_of_rollouts</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                                              <span class="n">task_horizon</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
                                              <span class="n">dynamics_function</span><span class="o">=</span><span class="n">dynamics_function</span><span class="p">)</span>
<span class="n">mpc_policy</span> <span class="o">=</span> <span class="n">MPCPolicy</span><span class="p">(</span><span class="n">reward_function</span><span class="o">=</span><span class="n">pendulum_reward_function</span><span class="p">,</span>
                       <span class="n">env_action_space</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="p">,</span>
                       <span class="n">env_observation_space</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="p">,</span>
                       <span class="n">dynamics_handler</span><span class="o">=</span><span class="n">dynamics_handler</span><span class="p">,</span>
                       <span class="n">optimizer_name</span><span class="o">=</span><span class="s1">&#39;CEM&#39;</span><span class="p">,</span>
                       <span class="n">num_agents</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="n">current_obs</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">action_to_execute</span><span class="p">,</span> <span class="n">expected_obs</span><span class="p">,</span> <span class="n">expected_reward</span> <span class="o">=</span> <span class="n">mpc_policy</span><span class="o">.</span><span class="n">act</span><span class="p">(</span>
        <span class="n">current_obs</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
    <span class="n">current_obs</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action_to_execute</span><span class="p">)</span>
    <span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">()</span>

<span class="n">dynamics_handler</span> <span class="o">=</span> <span class="n">learn_dynamics_from_policy</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">EnvironmentWrapper</span><span class="o">.</span><span class="n">make_standard_gym_env</span><span class="p">(</span><span class="s2">&quot;Pendulum-v0&quot;</span><span class="p">,</span>
                                                                                           <span class="n">num_of_agents</span><span class="o">=</span><span class="mi">10</span><span class="p">),</span>
                                              <span class="n">policy</span><span class="o">=</span><span class="n">mpc_policy</span><span class="p">,</span>
                                              <span class="n">number_of_rollouts</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                                              <span class="n">task_horizon</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
                                              <span class="n">system_dynamics_handler</span><span class="o">=</span><span class="n">dynamics_handler</span><span class="p">)</span>


<span class="n">current_obs</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">200</span><span class="p">):</span>
    <span class="n">action_to_execute</span><span class="p">,</span> <span class="n">expected_obs</span><span class="p">,</span> <span class="n">expected_reward</span> <span class="o">=</span> <span class="n">mpc_policy</span><span class="o">.</span><span class="n">act</span><span class="p">(</span>
        <span class="n">current_obs</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
    <span class="n">current_obs</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action_to_execute</span><span class="p">)</span>
    <span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="load-save-models">
<h2>Load/ Save Models<a class="headerlink" href="#load-save-models" title="Permalink to this headline">¶</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">- instantiate an env for a pendulum</span>
<span class="sd">- Define an MLP to learn a dynamics model</span>
<span class="sd">- instantiate a random policy to collect rollouts</span>
<span class="sd">- learn dynamics after collecting rollouts randomly</span>
<span class="sd">- save the model in the log dir with frequency 1.</span>
<span class="sd">- log everything in tensorboard.</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">blackbox_mpc.dynamics_functions.deterministic_mlp</span> <span class="kn">import</span> \
    <span class="n">DeterministicMLP</span>
<span class="kn">from</span> <span class="nn">blackbox_mpc.policies.random_policy</span> <span class="kn">import</span> <span class="n">RandomPolicy</span>
<span class="kn">from</span> <span class="nn">blackbox_mpc.utils.dynamics_learning</span> <span class="kn">import</span> <span class="n">learn_dynamics_from_policy</span>
<span class="kn">from</span> <span class="nn">blackbox_mpc.environment_utils</span> <span class="kn">import</span> <span class="n">EnvironmentWrapper</span>
<span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s2">&quot;Pendulum-v0&quot;</span><span class="p">)</span>
<span class="n">log_dir</span> <span class="o">=</span> <span class="s1">&#39;./&#39;</span>
<span class="n">tf_writer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">create_file_writer</span><span class="p">(</span><span class="n">log_dir</span><span class="p">)</span>
<span class="n">dynamics_function</span> <span class="o">=</span> <span class="n">DeterministicMLP</span><span class="p">(</span><span class="n">layers</span><span class="o">=</span><span class="p">[</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                                              <span class="mi">32</span><span class="p">,</span>
                                              <span class="mi">32</span><span class="p">,</span>
                                              <span class="mi">32</span><span class="p">,</span>
                                              <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span>
                                     <span class="n">activation_functions</span><span class="o">=</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">tanh</span><span class="p">,</span>
                                                           <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">tanh</span><span class="p">,</span>
                                                           <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">tanh</span><span class="p">,</span>
                                                           <span class="kc">None</span><span class="p">])</span>
<span class="n">policy</span> <span class="o">=</span> <span class="n">RandomPolicy</span><span class="p">(</span><span class="n">number_of_agents</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                      <span class="n">env_action_space</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="p">)</span>
<span class="n">dynamics_handler</span> <span class="o">=</span> <span class="n">learn_dynamics_from_policy</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">EnvironmentWrapper</span><span class="o">.</span><span class="n">make_standard_gym_env</span><span class="p">(</span><span class="s2">&quot;Pendulum-v0&quot;</span><span class="p">,</span>
                                                                                           <span class="n">num_of_agents</span><span class="o">=</span><span class="mi">10</span><span class="p">),</span>
                                              <span class="n">policy</span><span class="o">=</span><span class="n">policy</span><span class="p">,</span>
                                              <span class="n">number_of_rollouts</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                                              <span class="n">task_horizon</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
                                              <span class="n">dynamics_function</span><span class="o">=</span><span class="n">dynamics_function</span><span class="p">,</span>
                                              <span class="n">log_dir</span><span class="o">=</span><span class="n">log_dir</span><span class="p">,</span>
                                              <span class="n">tf_writer</span><span class="o">=</span><span class="n">tf_writer</span><span class="p">)</span>


</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">- instantiate an env for a pendulum</span>
<span class="sd">- instantiate an MPC by loading a previosuluy saved dynamics model.</span>
<span class="sd">- render the result.</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">blackbox_mpc.policies.mpc_policy</span> <span class="kn">import</span> \
    <span class="n">MPCPolicy</span>
<span class="kn">from</span> <span class="nn">blackbox_mpc.utils.pendulum</span> <span class="kn">import</span> <span class="n">pendulum_reward_function</span>
<span class="kn">import</span> <span class="nn">gym</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s2">&quot;Pendulum-v0&quot;</span><span class="p">)</span>
<span class="n">mpc_policy</span> <span class="o">=</span> <span class="n">MPCPolicy</span><span class="p">(</span><span class="n">reward_function</span><span class="o">=</span><span class="n">pendulum_reward_function</span><span class="p">,</span>
                       <span class="n">env_action_space</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="p">,</span>
                       <span class="n">env_observation_space</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="p">,</span>
                       <span class="n">optimizer_name</span><span class="o">=</span><span class="s1">&#39;CEM&#39;</span><span class="p">,</span>
                       <span class="n">saved_model_dir</span><span class="o">=</span><span class="s1">&#39;./saved_model&#39;</span><span class="p">,</span>
                       <span class="n">num_agents</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">current_obs</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">200</span><span class="p">):</span>
    <span class="n">action_to_execute</span><span class="p">,</span> <span class="n">expected_obs</span><span class="p">,</span> <span class="n">expected_reward</span> <span class="o">=</span> <span class="n">mpc_policy</span><span class="o">.</span><span class="n">act</span><span class="p">(</span>
        <span class="n">current_obs</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
    <span class="n">current_obs</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action_to_execute</span><span class="p">)</span>
    <span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="record-videos">
<h2>Record Videos<a class="headerlink" href="#record-videos" title="Permalink to this headline">¶</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">- instantiate an env for a pendulum</span>
<span class="sd">- instantiate an MPC controller using the true known analytical model</span>
<span class="sd">- record a rollout in a video.</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">blackbox_mpc.policies.mpc_policy</span> <span class="kn">import</span> \
    <span class="n">MPCPolicy</span>
<span class="kn">from</span> <span class="nn">blackbox_mpc.utils.pendulum</span> <span class="kn">import</span> <span class="n">PendulumTrueModel</span><span class="p">,</span> \
    <span class="n">pendulum_reward_function</span>
<span class="kn">from</span> <span class="nn">blackbox_mpc.utils.recording</span> <span class="kn">import</span> <span class="n">record_rollout</span>
<span class="kn">import</span> <span class="nn">gym</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s2">&quot;Pendulum-v0&quot;</span><span class="p">)</span>
<span class="n">mpc_policy</span> <span class="o">=</span> <span class="n">MPCPolicy</span><span class="p">(</span><span class="n">reward_function</span><span class="o">=</span><span class="n">pendulum_reward_function</span><span class="p">,</span>
                       <span class="n">env_action_space</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="p">,</span>
                       <span class="n">env_observation_space</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="p">,</span>
                       <span class="n">true_model</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                       <span class="n">dynamics_function</span><span class="o">=</span><span class="n">PendulumTrueModel</span><span class="p">(),</span>
                       <span class="n">optimizer_name</span><span class="o">=</span><span class="s1">&#39;CMA-ES&#39;</span><span class="p">,</span>
                       <span class="n">num_agents</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">record_rollout</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">horizon</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">policy</span><span class="o">=</span><span class="n">mpc_policy</span><span class="p">,</span>
               <span class="n">record_file_path</span><span class="o">=</span><span class="s1">&#39;./cma&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="using-low-level-api">
<h2>Using low-level API<a class="headerlink" href="#using-low-level-api" title="Permalink to this headline">¶</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">This tutorial is meant to show the modular structure of the package,</span>
<span class="sd">and the possibility of extending the functionality of each block further</span>
<span class="sd">if needed in your research.(such as new optimizer or</span>
<span class="sd">a new trajectory evaluator method..etc)</span>

<span class="sd">- instantiate an env for a pendulum.</span>
<span class="sd">- Define an MLP to learn a dynamics model</span>
<span class="sd">- Define the system handler that takes care of training the model</span>
<span class="sd">  and processing the rollouts..etc.</span>
<span class="sd">- Define a trajectory evaluator that evaluates the rewards of trajectories.</span>
<span class="sd">- Define an optimizer.</span>
<span class="sd">- instantiate a random policy to collect rollouts</span>
<span class="sd">- instantiate an mpc policy using the previous blocks.</span>
<span class="sd">- learn dynamics from random policy.</span>
<span class="sd">- use the learned dynamics with mpc and render the result</span>
<span class="sd">- record everything in tensorboard</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">blackbox_mpc.dynamics_functions.deterministic_mlp</span> <span class="kn">import</span> \
    <span class="n">DeterministicMLP</span>
<span class="kn">from</span> <span class="nn">blackbox_mpc.policies.random_policy</span> <span class="kn">import</span> <span class="n">RandomPolicy</span>
<span class="kn">from</span> <span class="nn">blackbox_mpc.utils.dynamics_learning</span> <span class="kn">import</span> <span class="n">learn_dynamics_from_policy</span>
<span class="kn">from</span> <span class="nn">blackbox_mpc.environment_utils</span> <span class="kn">import</span> <span class="n">EnvironmentWrapper</span>
<span class="kn">from</span> <span class="nn">blackbox_mpc.policies.mpc_policy</span> <span class="kn">import</span> \
    <span class="n">MPCPolicy</span>
<span class="kn">from</span> <span class="nn">blackbox_mpc.trajectory_evaluators.deterministic</span> <span class="kn">import</span> \
    <span class="n">DeterministicTrajectoryEvaluator</span>
<span class="kn">from</span> <span class="nn">blackbox_mpc.optimizers.cem</span> <span class="kn">import</span> <span class="n">CEMOptimizer</span>
<span class="kn">from</span> <span class="nn">blackbox_mpc.dynamics_handlers.system_dynamics_handler</span> <span class="kn">import</span> \
    <span class="n">SystemDynamicsHandler</span>
<span class="kn">from</span> <span class="nn">blackbox_mpc.utils.pendulum</span> <span class="kn">import</span> <span class="n">pendulum_reward_function</span>
<span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="n">log_dir</span> <span class="o">=</span> <span class="s1">&#39;./&#39;</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s2">&quot;Pendulum-v0&quot;</span><span class="p">)</span>
<span class="n">tf_writer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">create_file_writer</span><span class="p">(</span><span class="n">log_dir</span><span class="p">)</span>
<span class="n">dynamics_function</span> <span class="o">=</span> <span class="n">DeterministicMLP</span><span class="p">(</span><span class="n">layers</span><span class="o">=</span><span class="p">[</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                                              <span class="mi">32</span><span class="p">,</span>
                                              <span class="mi">32</span><span class="p">,</span>
                                              <span class="mi">32</span><span class="p">,</span>
                                              <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span>
                                     <span class="n">activation_functions</span><span class="o">=</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">tanh</span><span class="p">,</span>
                                                           <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">tanh</span><span class="p">,</span>
                                                           <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">tanh</span><span class="p">,</span>
                                                           <span class="kc">None</span><span class="p">])</span>

<span class="n">system_dynamics_handler</span> <span class="o">=</span> <span class="n">SystemDynamicsHandler</span><span class="p">(</span><span class="n">env_action_space</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="p">,</span>
                                                <span class="n">env_observation_space</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="p">,</span>
                                                <span class="n">true_model</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                                <span class="n">dynamics_function</span><span class="o">=</span><span class="n">dynamics_function</span><span class="p">,</span>
                                                <span class="n">tf_writer</span><span class="o">=</span><span class="n">tf_writer</span><span class="p">,</span>
                                                <span class="n">is_normalized</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                                <span class="n">log_dir</span><span class="o">=</span><span class="n">log_dir</span><span class="p">,</span>
                                                <span class="n">save_model_frequency</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">trajectory_evaluator</span> <span class="o">=</span> \
                    <span class="n">DeterministicTrajectoryEvaluator</span><span class="p">(</span><span class="n">reward_function</span><span class="o">=</span><span class="n">pendulum_reward_function</span><span class="p">,</span>
                                                     <span class="n">system_dynamics_handler</span><span class="o">=</span><span class="n">system_dynamics_handler</span><span class="p">)</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">CEMOptimizer</span><span class="p">(</span><span class="n">env_action_space</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="p">,</span>
                         <span class="n">env_observation_space</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="p">,</span>
                         <span class="n">num_agents</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                         <span class="n">planning_horizon</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span>
                         <span class="n">max_iterations</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>

<span class="n">policy</span> <span class="o">=</span> <span class="n">MPCPolicy</span><span class="p">(</span><span class="n">trajectory_evaluator</span><span class="o">=</span><span class="n">trajectory_evaluator</span><span class="p">,</span>
                   <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span>
                   <span class="n">tf_writer</span><span class="o">=</span><span class="n">tf_writer</span><span class="p">)</span>

<span class="n">random_policy</span> <span class="o">=</span> <span class="n">RandomPolicy</span><span class="p">(</span><span class="n">number_of_agents</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                      <span class="n">env_action_space</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="p">)</span>
<span class="n">dynamics_handler</span> <span class="o">=</span> <span class="n">learn_dynamics_from_policy</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">EnvironmentWrapper</span><span class="o">.</span><span class="n">make_standard_gym_env</span><span class="p">(</span><span class="s2">&quot;Pendulum-v0&quot;</span><span class="p">,</span>
                                                                                           <span class="n">num_of_agents</span><span class="o">=</span><span class="mi">10</span><span class="p">),</span>
                                              <span class="n">policy</span><span class="o">=</span><span class="n">random_policy</span><span class="p">,</span>
                                              <span class="n">number_of_rollouts</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                                              <span class="n">task_horizon</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
                                              <span class="n">system_dynamics_handler</span><span class="o">=</span><span class="n">system_dynamics_handler</span><span class="p">)</span>

<span class="n">current_obs</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">200</span><span class="p">):</span>
    <span class="n">action_to_execute</span><span class="p">,</span> <span class="n">expected_obs</span><span class="p">,</span> <span class="n">expected_reward</span> <span class="o">=</span> <span class="n">policy</span><span class="o">.</span><span class="n">act</span><span class="p">(</span>
        <span class="n">current_obs</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
    <span class="n">current_obs</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action_to_execute</span><span class="p">)</span>
    <span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="halfcheetah">
<h2>HalfCheetah<a class="headerlink" href="#halfcheetah" title="Permalink to this headline">¶</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">- instantiate an env for a modified version of halfcheetah mujoco env.</span>
<span class="sd">- Define an MLP to learn a dynamics model</span>
<span class="sd">- instantiate a random policy to collect rollouts</span>
<span class="sd">- learn dynamics in an iterative mpc fashion</span>
<span class="sd">(collect -&gt; learn model -&gt; collect using mpc with learned model -&gt; repeat)</span>
<span class="sd">- record everything in tensorboard</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">blackbox_mpc.dynamics_functions.deterministic_mlp</span> <span class="kn">import</span> \
    <span class="n">DeterministicMLP</span>
<span class="kn">from</span> <span class="nn">blackbox_mpc.policies.random_policy</span> <span class="kn">import</span> <span class="n">RandomPolicy</span>
<span class="kn">from</span> <span class="nn">blackbox_mpc.utils.iterative_mpc</span> <span class="kn">import</span> <span class="n">learn_dynamics_iteratively_w_mpc</span>
<span class="kn">from</span> <span class="nn">blackbox_mpc.environment_utils</span> <span class="kn">import</span> <span class="n">EnvironmentWrapper</span>
<span class="kn">from</span> <span class="nn">blackbox_mpc.utils.pendulum</span> <span class="kn">import</span> <span class="n">pendulum_reward_function</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">env_modified</span> <span class="kn">import</span> <span class="n">HalfCheetahEnvModified</span>
<span class="kn">from</span> <span class="nn">cost_func</span> <span class="kn">import</span> <span class="n">reward_function</span>

<span class="n">log_dir</span> <span class="o">=</span> <span class="s1">&#39;./&#39;</span>
<span class="n">tf_writer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">create_file_writer</span><span class="p">(</span><span class="n">log_dir</span><span class="p">)</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">HalfCheetahEnvModified</span><span class="p">()</span>
<span class="n">dynamics_function</span> <span class="o">=</span> <span class="n">DeterministicMLP</span><span class="p">(</span><span class="n">layers</span><span class="o">=</span><span class="p">[</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span>
                                             <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                                              <span class="mi">32</span><span class="p">,</span>
                                              <span class="mi">32</span><span class="p">,</span>
                                              <span class="mi">32</span><span class="p">,</span>
                                              <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span>
                                     <span class="n">activation_functions</span><span class="o">=</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">tanh</span><span class="p">,</span>
                                                           <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">tanh</span><span class="p">,</span>
                                                           <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">tanh</span><span class="p">,</span>
                                                           <span class="kc">None</span><span class="p">])</span>
<span class="n">initial_policy</span> <span class="o">=</span> <span class="n">RandomPolicy</span><span class="p">(</span><span class="n">number_of_agents</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                              <span class="n">env_action_space</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="p">)</span>

<span class="n">learn_dynamics_iteratively_w_mpc</span><span class="p">(</span><span class="n">env</span><span class="o">=</span><span class="n">EnvironmentWrapper</span><span class="o">.</span><span class="n">make_custom_gym_env</span><span class="p">(</span>
    <span class="n">HalfCheetahEnvModified</span><span class="p">,</span> <span class="n">num_of_agents</span><span class="o">=</span><span class="mi">10</span><span class="p">),</span>
                                 <span class="n">env_action_space</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="p">,</span>
                                 <span class="n">env_observation_space</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="p">,</span>
                                 <span class="n">number_of_initial_rollouts</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                                 <span class="n">number_of_rollouts_for_refinement</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                                 <span class="n">number_of_refinement_steps</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                                 <span class="n">task_horizon</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
                                 <span class="n">planning_horizon</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
                                 <span class="n">initial_policy</span><span class="o">=</span><span class="n">initial_policy</span><span class="p">,</span>
                                 <span class="n">dynamics_function</span><span class="o">=</span><span class="n">dynamics_function</span><span class="p">,</span>
                                 <span class="n">num_agents</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                                 <span class="n">reward_function</span><span class="o">=</span><span class="n">pendulum_reward_function</span><span class="p">,</span>
                                 <span class="n">log_dir</span><span class="o">=</span><span class="n">log_dir</span><span class="p">,</span>
                                 <span class="n">tf_writer</span><span class="o">=</span><span class="n">tf_writer</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">This tutorial is meant to show how to train a NN dynamics with MPC for the</span>
<span class="sd">cheetah environment.</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">blackbox_mpc.dynamics_functions.deterministic_mlp</span> <span class="kn">import</span> \
    <span class="n">DeterministicMLP</span>
<span class="kn">from</span> <span class="nn">blackbox_mpc.policies.random_policy</span> <span class="kn">import</span> <span class="n">RandomPolicy</span>
<span class="kn">from</span> <span class="nn">env_modified</span> <span class="kn">import</span> <span class="n">HalfCheetahEnvModified</span>
<span class="kn">from</span> <span class="nn">cost_func</span> <span class="kn">import</span> <span class="n">reward_function</span>
<span class="kn">from</span> <span class="nn">blackbox_mpc.utils.iterative_mpc</span> <span class="kn">import</span> <span class="n">learn_dynamics_iteratively_w_mpc</span>
<span class="kn">from</span> <span class="nn">blackbox_mpc.environment_utils</span> <span class="kn">import</span> <span class="n">EnvironmentWrapper</span>
<span class="kn">from</span> <span class="nn">blackbox_mpc.utils.recording</span> <span class="kn">import</span> <span class="n">record_rollout</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="n">log_dir</span> <span class="o">=</span> <span class="s1">&#39;./&#39;</span>
<span class="n">tf_writer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">summary</span><span class="o">.</span><span class="n">create_file_writer</span><span class="p">(</span><span class="n">log_dir</span><span class="p">)</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">HalfCheetahEnvModified</span><span class="p">()</span>
<span class="n">num_of_agents</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">parallel_env</span> <span class="o">=</span> <span class="n">EnvironmentWrapper</span><span class="o">.</span><span class="n">make_custom_gym_env</span><span class="p">(</span>
                                     <span class="n">HalfCheetahEnvModified</span><span class="p">,</span>
                                     <span class="n">num_of_agents</span><span class="o">=</span><span class="n">num_of_agents</span><span class="p">)</span>

<span class="n">dynamics_function</span> <span class="o">=</span> <span class="n">DeterministicMLP</span><span class="p">(</span><span class="n">layers</span><span class="o">=</span><span class="p">[</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span>
                                             <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                                             <span class="mi">500</span><span class="p">,</span>
                                             <span class="mi">500</span><span class="p">,</span>
                                             <span class="mi">500</span><span class="p">,</span>
                                             <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span>
                                     <span class="n">activation_functions</span><span class="o">=</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">tanh</span><span class="p">,</span>
                                                           <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">tanh</span><span class="p">,</span>
                                                           <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">tanh</span><span class="p">,</span>
                                                           <span class="kc">None</span><span class="p">])</span>
<span class="n">initial_policy</span> <span class="o">=</span> <span class="n">RandomPolicy</span><span class="p">(</span><span class="n">number_of_agents</span><span class="o">=</span><span class="n">num_of_agents</span><span class="p">,</span>
                              <span class="n">env_action_space</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="p">)</span>

<span class="n">system_dynamics_handler</span><span class="p">,</span> <span class="n">mpc_policy</span> <span class="o">=</span> <span class="n">learn_dynamics_iteratively_w_mpc</span><span class="p">(</span>
                                 <span class="n">env</span><span class="o">=</span><span class="n">parallel_env</span><span class="p">,</span>
                                 <span class="n">env_action_space</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="p">,</span>
                                 <span class="n">env_observation_space</span><span class="o">=</span><span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="p">,</span>
                                 <span class="n">number_of_initial_rollouts</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                                 <span class="n">number_of_rollouts_for_refinement</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                                 <span class="n">number_of_refinement_steps</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                 <span class="n">task_horizon</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
                                 <span class="n">planning_horizon</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span>
                                 <span class="n">initial_policy</span><span class="o">=</span><span class="n">initial_policy</span><span class="p">,</span>
                                 <span class="n">dynamics_function</span><span class="o">=</span><span class="n">dynamics_function</span><span class="p">,</span>
                                 <span class="n">num_agents</span><span class="o">=</span><span class="n">num_of_agents</span><span class="p">,</span>
                                 <span class="n">reward_function</span><span class="o">=</span><span class="n">reward_function</span><span class="p">,</span>
                                 <span class="n">log_dir</span><span class="o">=</span><span class="n">log_dir</span><span class="p">,</span>
                                 <span class="n">tf_writer</span><span class="o">=</span><span class="n">tf_writer</span><span class="p">,</span>
                                 <span class="n">optimizer_name</span><span class="o">=</span><span class="s1">&#39;RandomSearch&#39;</span><span class="p">,</span>
                                 <span class="n">population_size</span><span class="o">=</span><span class="mi">4048</span><span class="p">,</span>
                                 <span class="n">save_model_frequency</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                                 <span class="n">batch_size</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
                                 <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

<span class="n">record_rollout</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">horizon</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">policy</span><span class="o">=</span><span class="n">mpc_policy</span><span class="p">,</span>
               <span class="n">record_file_path</span><span class="o">=</span><span class="s1">&#39;./current_policy_0&#39;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">9</span><span class="p">):</span>
    <span class="n">system_dynamics_handler</span><span class="p">,</span> <span class="n">mpc_policy</span> <span class="o">=</span> <span class="n">learn_dynamics_iteratively_w_mpc</span><span class="p">(</span>
                                     <span class="n">env</span><span class="o">=</span><span class="n">parallel_env</span><span class="p">,</span>
                                     <span class="n">number_of_initial_rollouts</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                                     <span class="n">number_of_rollouts_for_refinement</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                                     <span class="n">number_of_refinement_steps</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                                     <span class="n">refinement_policy</span><span class="o">=</span><span class="n">mpc_policy</span><span class="p">,</span>
                                     <span class="n">task_horizon</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
                                     <span class="n">system_dynamics_handler</span><span class="o">=</span><span class="n">system_dynamics_handler</span><span class="p">,</span>
                                     <span class="n">batch_size</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span>
                                     <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                                     <span class="n">tf_writer</span><span class="o">=</span><span class="n">tf_writer</span><span class="p">,</span>
                                     <span class="n">start_episode</span><span class="o">=</span><span class="mi">3</span><span class="o">+</span><span class="p">(</span><span class="n">i</span><span class="o">*</span><span class="mi">5</span><span class="o">*</span><span class="mi">3</span><span class="p">))</span>
    <span class="n">record_rollout</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">horizon</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">policy</span><span class="o">=</span><span class="n">mpc_policy</span><span class="p">,</span>
                   <span class="n">record_file_path</span><span class="o">=</span><span class="s1">&#39;./current_policy_&#39;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="section" id="more">
<h2>More<a class="headerlink" href="#more" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="https://github.com/ossamaAhmed/blackbox_mpc/tree/master/tutorials">here</a></p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../modules/blackbox_mpc.html" class="btn btn-neutral float-right" title="blackbox_mpc" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="install.html" class="btn btn-neutral float-left" title="Installation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2020, Ossama Ahmed and Jonas Ruthfuss

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>